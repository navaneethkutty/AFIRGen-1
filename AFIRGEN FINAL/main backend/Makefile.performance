# Makefile for Performance Testing
#
# This Makefile provides convenient targets for running performance tests
# and generating reports.
#
# Requirements: 9.7

.PHONY: help perf-test perf-api perf-db perf-cache perf-load perf-report perf-baseline perf-clean

# Default target
help:
	@echo "Performance Testing Targets:"
	@echo "  make perf-test        - Run all performance benchmarks"
	@echo "  make perf-api         - Run API endpoint benchmarks only"
	@echo "  make perf-db          - Run database query benchmarks only"
	@echo "  make perf-cache       - Run cache operation benchmarks only"
	@echo "  make perf-load        - Run load tests with Locust"
	@echo "  make perf-report      - Generate performance reports"
	@echo "  make perf-baseline    - Save current results as baseline"
	@echo "  make perf-compare     - Compare current results to baseline"
	@echo "  make perf-clean       - Clean performance test artifacts"
	@echo ""
	@echo "Environment Variables:"
	@echo "  BASELINE_FILE         - Path to baseline metrics file (default: performance_baseline.json)"
	@echo "  OUTPUT_DIR            - Output directory for reports (default: performance_reports)"
	@echo "  FAIL_ON_REGRESSION    - Fail if tests exceed thresholds (default: false)"

# Configuration
BASELINE_FILE ?= performance_baseline.json
OUTPUT_DIR ?= performance_reports
FAIL_ON_REGRESSION ?= false

# Python and pytest
PYTHON := python3
PYTEST := pytest -c pytest.performance.ini
PERF_SCRIPT := scripts/generate_performance_report.py

# Run all performance benchmarks
perf-test:
	@echo "Running all performance benchmarks..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTEST) -m performance tests/performance/
	@echo "✓ Performance tests complete"

# Run API endpoint benchmarks
perf-api:
	@echo "Running API endpoint benchmarks..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTEST) -m "performance and api" tests/performance/test_api_benchmarks.py
	@echo "✓ API benchmarks complete"

# Run database query benchmarks
perf-db:
	@echo "Running database query benchmarks..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTEST) -m "performance and database" tests/performance/test_database_benchmarks.py
	@echo "✓ Database benchmarks complete"

# Run cache operation benchmarks
perf-cache:
	@echo "Running cache operation benchmarks..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTEST) -m "performance and cache" tests/performance/test_cache_benchmarks.py
	@echo "✓ Cache benchmarks complete"

# Run load tests with Locust
perf-load:
	@echo "Running load tests with Locust..."
	@mkdir -p $(OUTPUT_DIR)
	@if [ ! -f locustfile.py ]; then \
		echo "Creating default locustfile.py..."; \
		cat > locustfile.py << 'EOF'; \
from locust import HttpUser, task, between; \
; \
class AFIRGenUser(HttpUser):; \
    wait_time = between(1, 3); \
    ; \
    @task(3); \
    def get_session_status(self):; \
        self.client.get("/api/v1/session/test_session_123/status"); \
    ; \
    @task(2); \
    def get_fir_status(self):; \
        self.client.get("/api/v1/fir/FIR_123"); \
    ; \
    @task(1); \
    def validate_step(self):; \
        self.client.post("/api/v1/validate", json={"session_id": "test_session_123", "approved": True}); \
EOF; \
	fi
	locust -f locustfile.py \
		--headless \
		--users 15 \
		--spawn-rate 5 \
		--run-time 5m \
		--host http://localhost:8000 \
		--html $(OUTPUT_DIR)/load_test_report.html \
		--csv $(OUTPUT_DIR)/load_test_results
	@echo "✓ Load tests complete"

# Generate performance reports
perf-report:
	@echo "Generating performance reports..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTHON) $(PERF_SCRIPT) \
		--baseline $(BASELINE_FILE) \
		--output-dir $(OUTPUT_DIR)
	@echo "✓ Reports generated in $(OUTPUT_DIR)/"
	@echo ""
	@cat $(OUTPUT_DIR)/performance_summary.txt

# Run tests and generate reports
perf-full:
	@echo "Running full performance test suite..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTHON) $(PERF_SCRIPT) \
		--run-tests \
		--baseline $(BASELINE_FILE) \
		--output-dir $(OUTPUT_DIR) \
		$(if $(filter true,$(FAIL_ON_REGRESSION)),--fail-on-regression,)
	@echo "✓ Full performance test suite complete"

# Save current results as baseline
perf-baseline:
	@echo "Saving current results as baseline..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTHON) $(PERF_SCRIPT) \
		--save-baseline \
		--baseline $(BASELINE_FILE) \
		--output-dir $(OUTPUT_DIR)
	@echo "✓ Baseline saved to $(BASELINE_FILE)"

# Compare current results to baseline
perf-compare:
	@echo "Comparing current results to baseline..."
	@if [ ! -f $(BASELINE_FILE) ]; then \
		echo "✗ No baseline found at $(BASELINE_FILE)"; \
		echo "  Run 'make perf-baseline' to create a baseline first"; \
		exit 1; \
	fi
	@mkdir -p $(OUTPUT_DIR)
	$(PYTHON) $(PERF_SCRIPT) \
		--run-tests \
		--baseline $(BASELINE_FILE) \
		--output-dir $(OUTPUT_DIR)
	@echo "✓ Comparison complete. Check $(OUTPUT_DIR)/performance_summary.txt"

# Clean performance test artifacts
perf-clean:
	@echo "Cleaning performance test artifacts..."
	rm -rf $(OUTPUT_DIR)
	rm -f locustfile.py
	rm -f *.log
	rm -f .pytest_cache
	find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	@echo "✓ Cleanup complete"

# CI/CD target - runs tests and fails on regression
perf-ci:
	@echo "Running performance tests for CI/CD..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTHON) $(PERF_SCRIPT) \
		--run-tests \
		--baseline $(BASELINE_FILE) \
		--output-dir $(OUTPUT_DIR) \
		--fail-on-regression
	@echo "✓ CI/CD performance tests passed"

# Quick smoke test - runs a subset of fast tests
perf-smoke:
	@echo "Running performance smoke tests..."
	@mkdir -p $(OUTPUT_DIR)
	$(PYTEST) -m "performance and not slow" tests/performance/ -k "simple or get"
	@echo "✓ Smoke tests complete"

# Watch mode - continuously run tests on file changes
perf-watch:
	@echo "Watching for changes and running performance tests..."
	@which pytest-watch > /dev/null || (echo "Installing pytest-watch..." && pip install pytest-watch)
	pytest-watch -c pytest.performance.ini tests/performance/

# Install performance testing dependencies
perf-install:
	@echo "Installing performance testing dependencies..."
	pip install pytest pytest-benchmark locust hypothesis
	@echo "✓ Dependencies installed"

# Show performance test statistics
perf-stats:
	@echo "Performance Test Statistics:"
	@echo "----------------------------"
	@echo "Total test files: $$(find tests/performance -name 'test_*.py' | wc -l)"
	@echo "Total test functions: $$(grep -r "def test_" tests/performance | wc -l)"
	@if [ -f $(BASELINE_FILE) ]; then \
		echo "Baseline exists: Yes ($(BASELINE_FILE))"; \
		echo "Baseline tests: $$(cat $(BASELINE_FILE) | grep -o '"test_name"' | wc -l)"; \
	else \
		echo "Baseline exists: No"; \
	fi
	@if [ -d $(OUTPUT_DIR) ]; then \
		echo "Reports directory: $(OUTPUT_DIR)"; \
		echo "Report files: $$(ls -1 $(OUTPUT_DIR) 2>/dev/null | wc -l)"; \
	fi
